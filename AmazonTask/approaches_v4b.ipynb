{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04c3659-fbb6-42c2-88df-32d734c3440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 26 19:48:11 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:A1:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             43W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65d6de7-9626-422c-bd12-1bba9293b50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49d26ee93e048c683ee7d130d112765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer \n",
    "import time\n",
    "import torch\n",
    "import json\n",
    "\n",
    "llama3_model = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device_map=\"auto\",)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd8daab-97c6-48af-834c-db24969706da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a title for each review using the combined file\n",
    "def generate_title_for_reviews(data, max_input_length, max_output_length, tokenizer, k, mode=\"both\", task=None):\n",
    "    results = []\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process the dataset item by item\n",
    "    for item in tqdm(data, desc=\"Processing Reviews\"):\n",
    "        result = process_item(item, max_input_length, max_output_length, tokenizer, k, mode=mode, task=task)\n",
    "        results.append(result)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to generate titles for mode '{mode}': {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process a single item (user's review) from the combined JSON file\n",
    "def process_item(item, max_input_length, max_output_length, tokenizer, k, mode=\"both\", task=None):\n",
    "    example_user_id = item['user_id']\n",
    "    example_product_id = item['product_id']\n",
    "\n",
    "    # Adjust the query based on the task type (e.g., title, review, or both)\n",
    "    if task == \"title\":\n",
    "        query = item.get('user_review_title', '')  # Use the review title as the query\n",
    "    elif task == \"review\":\n",
    "        query = item.get('user_review_text', '')  # Use the review text as the query\n",
    "\n",
    "    # Retrieve the pre-ranked data for the user from the combined file\n",
    "    user_ratings = item.get('user_ratings', [])[:k]  \n",
    "    neighbor_ratings = item.get('neighbor_ratings', [])[:k]  \n",
    "    all_ratings = item.get('all_ratings', [])[:k]  \n",
    "\n",
    "    # Construct the prompt using the top-k user and neighbor ratings\n",
    "    prompt = tokenized_prompt(user_ratings, neighbor_ratings, query, max_input_length, tokenizer, mode=mode, task=task)\n",
    "    \n",
    "    # Generate text using the Llama 3 model\n",
    "    generated_text = llama3_model(prompt, max_new_tokens=max_output_length, do_sample=True, return_full_text=False)\n",
    "    \n",
    "    # Extract the generated title\n",
    "    title = generated_text[0]['generated_text'].strip()\n",
    "\n",
    "    return {\"user_id\": item['user_id'], \"product_id\": item['product_id'], \"output\": title}\n",
    "\n",
    "# Tokenized prompt function\n",
    "def tokenized_prompt(user_ratings, neighbor_ratings, inp, max_input_length, tokenizer, mode=\"both\", task=\"title\"):\n",
    "    user_contexts = []\n",
    "    neighbor_contexts = []\n",
    "\n",
    "    # Create user review context if mode is 'both' or 'user'\n",
    "    if mode in [\"both\", \"user\"]:\n",
    "        for idx, review in enumerate(user_ratings, start=1):\n",
    "            context = f\"User's Product {idx} Review: Review text: \\\"{review['reviewText']}\\\", Review title: \\\"{review['reviewTitle']}\\\"\"\n",
    "            tokens = tokenizer(context, max_length=max_input_length, truncation=True)\n",
    "            user_contexts.append(tokenizer.batch_decode([tokens['input_ids']], skip_special_tokens=True)[0])\n",
    "\n",
    "    # Create neighbor review context if mode is 'both' or 'neighbor'\n",
    "    if mode in [\"all\", \"both\", \"neighbor\"]:\n",
    "        for idx, neighbor in enumerate(neighbor_ratings, start=1):\n",
    "            context = f\"User {idx} Product Review: Review text: \\\"{neighbor['reviewText']}\\\", Review title: \\\"{neighbor['reviewTitle']}\\\"\"\n",
    "            tokens = tokenizer(context, max_length=max_input_length, truncation=True)\n",
    "            neighbor_contexts.append(tokenizer.batch_decode([tokens['input_ids']], skip_special_tokens=True)[0])\n",
    "\n",
    "    # Combine contexts based on mode\n",
    "    combined_contexts = []\n",
    "    if mode in [\"both\", \"user\"]:\n",
    "        combined_contexts.append(\"User's Own Reviews:\\n\")\n",
    "        combined_contexts.extend(user_contexts)\n",
    "    if mode in [\"all\", \"both\", \"neighbor\"]:\n",
    "        combined_contexts.append(\"\\nOther Users' Reviews:\\n\")\n",
    "        combined_contexts.extend(neighbor_contexts)\n",
    "\n",
    "    combined_context_str = \"\\n\".join(combined_contexts)\n",
    "    \n",
    "    # Custom prompting words based on mode\n",
    "    if mode == \"both\":\n",
    "        intro = \"Given the following reviews from the same user and other users on the same product:\\n\"\n",
    "    elif mode == \"all\":\n",
    "        intro = \"Given the following reviews from any user on any product:\\n\"\n",
    "    elif mode == \"user\":\n",
    "        intro = \"Given the following reviews from the user on different products:\\n\"\n",
    "    elif mode == \"neighbor\":\n",
    "        intro = \"Given the following reviews from other users on the same product:\\n\"\n",
    "\n",
    "    if task == \"title\": # the \"original\" directions\n",
    "        direction = \"\\nGenerate a title for the following product review from this user without any explanation: Review:\"\n",
    "        gen_direction = \"Generate the answer in 10 words or less using the format: 'The title is:'.\\n\"\n",
    "    elif task == \"review\":\n",
    "        direction = \"\\nGenerate a review for the following product from this user given the review title, without any explanation: Title:\"\n",
    "        gen_direction = \"Generate a review using the format: 'The review text is: '.\\n\"\n",
    "\n",
    "    # Final prompt with specific instruction\n",
    "    combined_prompt = (\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\"\n",
    "        f\"{intro}\"\n",
    "        f\"{combined_context_str}.\\n\"\n",
    "        f\"{direction} \\\"{inp}\\\".\\n\"\n",
    "        f\"{gen_direction}\"\n",
    "        f\"Do NOT generate anything else!.\"\n",
    "        f\"<<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "    \n",
    "    return combined_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92a8059-bf25-48d1-b38d-de00e3869e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "<|start_header_id|>user<|end_header_id|>Given the following reviews from the same user and other users on the same product:\n",
      "User's Own Reviews:\n",
      "\n",
      "User's Product 1 Review: Review text: \"Loved the product, great value!\", Review title: \"Excellent choice\"\n",
      "User's Product 2 Review: Review text: \"Superb quality, will buy again.\", Review title: \"Worth the money\"\n",
      "\n",
      "Other Users' Reviews:\n",
      "\n",
      "User 1 Product Review: Review text: \"Very satisfied with the product quality.\", Review title: \"Good buy\"\n",
      "User 2 Product Review: Review text: \"It works, but expected better quality.\", Review title: \"Average product\".\n",
      "\n",
      "Generate a review for the following product from this user given the review title, without any explanation: Title: \"This is a great product, highly recommend!\".\n",
      "Generate a review around the user's review length using the format: 'The review text is: '.\n",
      "Do NOT generate anything else!.<<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "Generated Title:\n",
      "The review text is: \"Loved the product, great value!\"\n"
     ]
    }
   ],
   "source": [
    "# Define mock data for testing\n",
    "mock_item = {\n",
    "    \"user_id\": \"TEST_USER_123\",\n",
    "    \"product_id\": \"TEST_PRODUCT_456\",\n",
    "    \"user_review_text\": \"This is a great product, highly recommend!\",\n",
    "    \"user_ratings\": [\n",
    "        {\"reviewTitle\": \"Excellent choice\", \"reviewText\": \"Loved the product, great value!\"},\n",
    "        {\"reviewTitle\": \"Worth the money\", \"reviewText\": \"Superb quality, will buy again.\"}\n",
    "    ],\n",
    "    \"neighbor_ratings\": [\n",
    "        {\"reviewTitle\": \"Good buy\", \"reviewText\": \"Very satisfied with the product quality.\"},\n",
    "        {\"reviewTitle\": \"Average product\", \"reviewText\": \"It works, but expected better quality.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Set parameters\n",
    "max_input_length = 512\n",
    "max_output_length = 218\n",
    "k = 2  # Take the top-2 ratings\n",
    "mode = \"both\"  # Use both user and neighbor reviews\n",
    "task = \"review\"\n",
    "# Test the tokenized prompt\n",
    "prompt = tokenized_prompt(\n",
    "    user_ratings=mock_item['user_ratings'],\n",
    "    neighbor_ratings=mock_item['neighbor_ratings'],\n",
    "    inp=mock_item['user_review_text'],\n",
    "    max_input_length=max_input_length,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=mode,\n",
    "    task=task\n",
    ")\n",
    "\n",
    "# Print the generated prompt to verify it\n",
    "print(\"Generated Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "# Generate text using the Llama 3 model\n",
    "generated_text = llama3_model(prompt, max_new_tokens=max_output_length, do_sample=True, return_full_text=False)\n",
    "\n",
    "# Extract the generated title\n",
    "title = generated_text[0]['generated_text'].strip()\n",
    "\n",
    "# Print the generated title\n",
    "print(\"Generated Title:\")\n",
    "print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879b8cef-0422-4d4e-ab08-0b67f1877133",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # if mode in [\"both\", \"user\"]:\n",
    "    #     for idx, review in enumerate(user_ratings, start=1):\n",
    "    #         context = \"User's Product {} Review: Review text: \\\"{}\\\", Review title: \\\"{}\\\"\".format(\n",
    "    #             idx, review[\"reviewText\"], review.get(\"reviewTitle\", \"No title available\"),\n",
    "    #         )\n",
    "    #         tokens = tokenizer(context, max_length=max_input_length, truncation=True)\n",
    "    #         user_contexts.append(tokenizer.batch_decode([tokens['input_ids']], skip_special_tokens=True)[0])\n",
    "\n",
    "    # # Create neighbor review context if mode is 'both', 'neighbor', or 'all'\n",
    "    # if mode in [\"all\", \"both\", \"neighbor\"]:\n",
    "    #     for idx, neighbor in enumerate(neighbor_ratings, start=1):\n",
    "    #         context = \"User {} Product Review: Review text: \\\"{}\\\", Review title: \\\"{}\\\"\".format(\n",
    "    #             idx, neighbor[\"reviewText\"], neighbor.get(\"reviewTitle\", \"No title available\"),\n",
    "    #         )\n",
    "    #         tokens = tokenizer(context, max_length=max_input_length, truncation=True)\n",
    "    #         neighbor_contexts.append(tokenizer.batch_decode([tokens['input_ids']], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302eb03c-c79c-4a28-93c5-6e8591e86c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate titles for multiple modes and k-values\n",
    "def generate_title_for_all_modes(data, max_input_length, max_output_length, tokenizer, k_values=[2, 3, 5]):\n",
    "    modes = [\"all\", \"both\", \"user\", \"neighbor\"]\n",
    "    task=\"review\"\n",
    "    for k in k_values:\n",
    "        for mode in modes:\n",
    "            print(f\"Processing mode: {mode} with k={k}\")\n",
    "            \n",
    "            # Generate titles for the current mode and k value\n",
    "            results = generate_title_for_reviews(\n",
    "                data, \n",
    "                max_input_length, \n",
    "                max_output_length, \n",
    "                tokenizer, \n",
    "                k=k,  # Now passing k for internal handling\n",
    "                mode=mode,\n",
    "                task=task\n",
    "            )\n",
    "            \n",
    "            # Define the output file name\n",
    "            output_file = f'results_test_{mode}_k{k}_review.json'\n",
    "            \n",
    "            # Save the results to a JSON file\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "            print(f\"Results for mode '{mode}' with k={k} have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea482030-d353-4093-9320-e12ee7c9d354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mode: all with k=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reviews:   0%|                                        | 0/2500 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 1/2500 [00:00<37:50,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 2/2500 [00:01<34:43,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 3/2500 [00:02<38:15,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 4/2500 [00:03<41:12,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 5/2500 [00:05<48:13,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 6/2500 [00:06<54:01,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 7/2500 [00:07<45:41,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 8/2500 [00:08<42:07,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                                | 9/2500 [00:09<43:05,  1.04s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|                               | 10/2500 [00:10<44:29,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|▏                              | 11/2500 [00:11<44:26,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   0%|▏                              | 12/2500 [00:12<42:44,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                              | 13/2500 [00:13<38:52,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                              | 14/2500 [00:14<42:11,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                              | 15/2500 [00:15<43:46,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                              | 16/2500 [00:17<55:30,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                              | 17/2500 [00:19<57:33,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                            | 18/2500 [00:22<1:17:33,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                            | 19/2500 [00:23<1:13:26,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                            | 20/2500 [00:24<1:01:59,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▏                            | 21/2500 [00:27<1:16:00,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 22/2500 [00:29<1:22:33,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 23/2500 [00:32<1:29:02,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 24/2500 [00:33<1:21:24,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 25/2500 [00:34<1:11:01,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 26/2500 [00:36<1:05:51,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 27/2500 [00:37<1:09:26,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 28/2500 [00:40<1:17:59,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                            | 29/2500 [00:41<1:03:37,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▎                              | 30/2500 [00:41<50:59,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▍                              | 31/2500 [00:43<55:44,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▍                              | 32/2500 [00:44<48:35,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▍                              | 33/2500 [00:44<42:23,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▍                            | 34/2500 [00:47<1:02:25,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▍                              | 35/2500 [00:48<52:47,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▍                              | 36/2500 [00:49<47:59,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   1%|▍                              | 37/2500 [00:49<40:56,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▍                              | 38/2500 [00:52<59:52,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▍                              | 39/2500 [00:52<47:24,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▍                              | 40/2500 [00:53<39:55,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▌                              | 41/2500 [00:53<38:29,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▌                              | 42/2500 [00:54<37:31,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▋                            | 56/2500 [01:18<1:09:53,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▋                              | 57/2500 [01:19<57:33,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▋                              | 58/2500 [01:19<49:13,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▋                              | 59/2500 [01:20<41:47,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▋                              | 60/2500 [01:22<49:57,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▊                              | 61/2500 [01:22<39:13,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   2%|▊                              | 62/2500 [01:23<37:13,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 63/2500 [01:24<43:17,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 64/2500 [01:26<52:01,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 65/2500 [01:28<55:32,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 66/2500 [01:28<43:41,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 67/2500 [01:29<36:19,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 68/2500 [01:29<31:48,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 69/2500 [01:30<29:57,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing Reviews:   3%|▊                              | 69/2500 [01:30<52:59,  1.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m ranked_data \u001b[38;5;241m=\u001b[39m load_data(ranked_test_file)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Call the function to process the data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m generate_title_for_all_modes(\n\u001b[1;32m     11\u001b[0m     data\u001b[38;5;241m=\u001b[39mranked_data,\n\u001b[1;32m     12\u001b[0m     max_input_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     13\u001b[0m     max_output_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     14\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     15\u001b[0m     k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     16\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mgenerate_title_for_all_modes\u001b[0;34m(data, max_input_length, max_output_length, tokenizer, k_values)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate titles for the current mode and k value\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m generate_title_for_reviews(\n\u001b[1;32m     11\u001b[0m     data, \n\u001b[1;32m     12\u001b[0m     max_input_length, \n\u001b[1;32m     13\u001b[0m     max_output_length, \n\u001b[1;32m     14\u001b[0m     tokenizer, \n\u001b[1;32m     15\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,  \u001b[38;5;66;03m# Now passing k for internal handling\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m     17\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Define the output file name\u001b[39;00m\n\u001b[1;32m     21\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_test_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_review.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mgenerate_title_for_reviews\u001b[0;34m(data, max_input_length, max_output_length, tokenizer, k, mode, task)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Process the dataset item by item\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     result \u001b[38;5;241m=\u001b[39m process_item(item, max_input_length, max_output_length, tokenizer, k, mode\u001b[38;5;241m=\u001b[39mmode, task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[1;32m     11\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# End timing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mprocess_item\u001b[0;34m(item, max_input_length, max_output_length, tokenizer, k, mode, task)\u001b[0m\n\u001b[1;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenized_prompt(user_ratings, neighbor_ratings, query, max_input_length, tokenizer, mode\u001b[38;5;241m=\u001b[39mmode, task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Generate text using the Llama 3 model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m llama3_model(prompt, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_output_length, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Extract the generated title\u001b[39;00m\n\u001b[1;32m     42\u001b[0m title \u001b[38;5;241m=\u001b[39m generated_text[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:262\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1264\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1164\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1163\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1164\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1165\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:351\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[1;32m    352\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2025\u001b[0m         input_ids,\n\u001b[1;32m   2026\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2027\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[1;32m   2028\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2029\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2030\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2031\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2032\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2033\u001b[0m     )\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1190\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1191\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1192\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1193\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1194\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1195\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1196\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1197\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1198\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1199\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1200\u001b[0m )\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1002\u001b[0m         hidden_states,\n\u001b[1;32m   1003\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1004\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1005\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1006\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1007\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1008\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1009\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m   1010\u001b[0m     )\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:734\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    735\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    736\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    737\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    738\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    739\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    740\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    741\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    742\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    744\u001b[0m )\n\u001b[1;32m    745\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:635\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 635\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:275\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    273\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    274\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 275\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    276\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:250\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    248\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    249\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;241m-\u001b[39mx2, x1), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Function to load data from a JSON file\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "ranked_test_file = \"../data/AmazonReview/amazon_title_generation_questions_test_ranked_k_5_reviewText.json\"\n",
    "ranked_data = load_data(ranked_test_file)\n",
    "\n",
    "# Call the function to process the data\n",
    "generate_title_for_all_modes(\n",
    "    data=ranked_data,\n",
    "    max_input_length=512,\n",
    "    max_output_length=256,\n",
    "    tokenizer=tokenizer,\n",
    "    k_values=[1, 2, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d54057-c822-4adb-83e7-5e606005934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate titles for multiple modes and k-values\n",
    "def generate_title_for_all_modes(data, max_input_length, max_output_length, tokenizer, k_values=[2, 3, 5]):\n",
    "    modes = [\"all\", \"both\", \"user\", \"neighbor\"]\n",
    "    task=\"review\"\n",
    "    for k in k_values:\n",
    "        for mode in modes:\n",
    "            print(f\"Processing mode: {mode} with k={k}\")\n",
    "            \n",
    "            # Generate titles for the current mode and k value\n",
    "            results = generate_title_for_reviews(\n",
    "                data, \n",
    "                max_input_length, \n",
    "                max_output_length, \n",
    "                tokenizer, \n",
    "                k=k,  # Now passing k for internal handling\n",
    "                mode=mode,\n",
    "                task=task\n",
    "            )\n",
    "            \n",
    "            # Define the output file name\n",
    "            output_file = f'results_dev_{mode}_k{k}_review.json'\n",
    "            \n",
    "            # Save the results to a JSON file\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            \n",
    "            print(f\"Results for mode '{mode}' with k={k} have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323a598-30da-4785-babf-7a0cc14fa8bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranked_dev_file = \"../data/AmazonReview/amazon_title_generation_questions_dev_ranked_k_5_reviewText.json\"\n",
    "ranked_data = load_data(ranked_dev_file)\n",
    "\n",
    "# Call the function to process the data\n",
    "generate_title_for_all_modes(\n",
    "    data=ranked_data,\n",
    "    max_input_length=512,\n",
    "    max_output_length=256,\n",
    "    tokenizer=tokenizer,\n",
    "    k_values=[1, 2, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4f6b6-2de2-4dd0-a832-64075683f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Button\n",
    "\n",
    "def shutdown_kernel():\n",
    "    from IPython.display import display\n",
    "    display(\"Shutting down kernel...\")\n",
    "    get_ipython().kernel.do_shutdown(True)\n",
    "\n",
    "shutdown_kernel()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
