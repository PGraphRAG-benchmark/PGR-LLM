{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff97494a-26f8-4955-9cd3-5121a0072326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the restructured JSON data\n",
    "with open('restructured_user_data.json', 'r') as json_file:\n",
    "    user_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffd61d6-67a0-4e3a-b721-acc9fb14288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_REVIEWS = 5  # Minimum number of reviews required\n",
    "\n",
    "# Filter users with enough reviews\n",
    "filtered_user_data = {user_id: data for user_id, data in user_data.items() if len(data['ratings']) >= MIN_REVIEWS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b678fd7a-3acc-4ceb-9abc-99000ab28f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get all user IDs\n",
    "user_ids = list(filtered_user_data.keys())\n",
    "\n",
    "# Split the user IDs into train, validation, and test sets\n",
    "train_ids, temp_ids = train_test_split(user_ids, test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create dictionaries for each split\n",
    "train_data = {user_id: filtered_user_data[user_id]['ratings'] for user_id in train_ids}\n",
    "val_data = {user_id: filtered_user_data[user_id]['ratings'] for user_id in val_ids}\n",
    "test_data = {user_id: filtered_user_data[user_id]['ratings'] for user_id in test_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a11c366-62f0-470f-a55e-e3b28ef3322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 35 movies in training set. Adjusting splits...\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique movie IDs\n",
    "all_movie_ids = set()\n",
    "for data in filtered_user_data.values():\n",
    "    for rating in data['ratings']:\n",
    "        all_movie_ids.add(rating['movie_id'])\n",
    "\n",
    "# Check if all movies are in the training set\n",
    "train_movie_ids = set()\n",
    "for ratings in train_data.values():\n",
    "    for rating in ratings:\n",
    "        train_movie_ids.add(rating['movie_id'])\n",
    "\n",
    "# If not all movies are in the training set, adjust the splits\n",
    "missing_movie_ids = all_movie_ids - train_movie_ids\n",
    "if missing_movie_ids:\n",
    "    print(f\"Missing {len(missing_movie_ids)} movies in training set. Adjusting splits...\")\n",
    "    # Adjust splits to include all movies in the training set\n",
    "    for movie_id in missing_movie_ids:\n",
    "        found = False\n",
    "        # First try to move from the test set to the train set\n",
    "        for user_id, ratings in list(test_data.items()):\n",
    "            for rating in ratings:\n",
    "                if rating['movie_id'] == movie_id:\n",
    "                    if user_id in train_data:\n",
    "                        train_data[user_id].append(rating)\n",
    "                    else:\n",
    "                        train_data[user_id] = [rating]\n",
    "                    test_data[user_id].remove(rating)\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            # If not found in test set, try to move from the validation set\n",
    "            for user_id, ratings in list(val_data.items()):\n",
    "                for rating in ratings:\n",
    "                    if rating['movie_id'] == movie_id:\n",
    "                        if user_id in train_data:\n",
    "                            train_data[user_id].append(rating)\n",
    "                        else:\n",
    "                            train_data[user_id] = [rating]\n",
    "                        val_data[user_id].remove(rating)\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d680e06-e267-4fc2-af52-29b8e6b72c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Combine titles and genres from all splits\n",
    "all_titles = [f['title'] for f in train_features + val_features + test_features]\n",
    "all_genres = [f['genres'] for f in train_features + val_features + test_features]\n",
    "\n",
    "# Fit the vectorizer on the entire corpus\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(all_titles + all_genres)\n",
    "\n",
    "# Function to prepare data for the model\n",
    "def prepare_model_data(features):\n",
    "    titles = [f['title'] for f in features]\n",
    "    genres = [f['genres'] for f in features]\n",
    "    \n",
    "    X_titles = vectorizer.transform(titles)\n",
    "    X_genres = vectorizer.transform(genres)\n",
    "    \n",
    "    # Combine these features as needed. Here we concatenate them.\n",
    "    X = hstack([X_titles, X_genres])\n",
    "    return X\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Prepare the data\n",
    "X_train = prepare_model_data(train_features)\n",
    "X_val = prepare_model_data(val_features)\n",
    "X_test = prepare_model_data(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63541356-c329-479d-a2ff-0b2f850176e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.2347046664679845\n",
      "Validation Score: 0.2125427093968647\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train a simple model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, train_targets)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = model.score(X_train, train_targets)\n",
    "val_score = model.score(X_val, val_targets)\n",
    "\n",
    "print(f'Train Score: {train_score}')\n",
    "print(f'Validation Score: {val_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f5836a0-f894-46e9-80b4-305837d8d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits saved to 'train_data.json', 'val_data.json', and 'test_data.json'\n"
     ]
    }
   ],
   "source": [
    "# Define NumpyEncoder to handle numpy data types\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, np.datetime64):\n",
    "            return obj.item().isoformat()\n",
    "        else:\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# Define a function to save data to JSON\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "# Save the train, validation, and test splits\n",
    "save_to_json(train_data, 'train_data.json')\n",
    "save_to_json(val_data, 'val_data.json')\n",
    "save_to_json(test_data, 'test_data.json')\n",
    "\n",
    "print(\"Data splits saved to 'train_data.json', 'val_data.json', and 'test_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128d811-8778-4f1c-9650-13e03cbdc816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
