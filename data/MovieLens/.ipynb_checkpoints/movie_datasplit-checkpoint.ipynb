{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd02439-124e-4cee-ab81-673f0e4d99c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 35 movies in training set. Adjusting splits...\n",
      "Data splits and prediction targets saved to JSON files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the restructured JSON data\n",
    "with open('restructured_user_data.json', 'r') as json_file:\n",
    "    user_data = json.load(json_file)\n",
    "\n",
    "MIN_REVIEWS = 5  # Minimum number of reviews required\n",
    "\n",
    "# Filter users with enough reviews\n",
    "filtered_user_data = {user_id: data for user_id, data in user_data.items() if len(data['ratings']) >= MIN_REVIEWS}\n",
    "\n",
    "# Get all user IDs\n",
    "user_ids = list(filtered_user_data.keys())\n",
    "\n",
    "# Split the user IDs into train, validation, and test sets\n",
    "train_ids, temp_ids = train_test_split(user_ids, test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create dictionaries for each split\n",
    "train_data = {user_id: filtered_user_data[user_id]['ratings'] for user_id in train_ids}\n",
    "val_data = {user_id: filtered_user_data[user_id]['ratings'] for user_id in val_ids}\n",
    "test_data = {user_id: filtered_user_data[user_id]['ratings'] for user_id in test_ids}\n",
    "\n",
    "# Ensure each movie is represented in the training set\n",
    "# Extract all unique movie IDs\n",
    "all_movie_ids = set()\n",
    "for data in filtered_user_data.values():\n",
    "    for rating in data['ratings']:\n",
    "        all_movie_ids.add(rating['movie_id'])\n",
    "\n",
    "# Check if all movies are in the training set\n",
    "train_movie_ids = set()\n",
    "for ratings in train_data.values():\n",
    "    for rating in ratings:\n",
    "        train_movie_ids.add(rating['movie_id'])\n",
    "\n",
    "# If not all movies are in the training set, adjust the splits\n",
    "missing_movie_ids = all_movie_ids - train_movie_ids\n",
    "if missing_movie_ids:\n",
    "    print(f\"Missing {len(missing_movie_ids)} movies in training set. Adjusting splits...\")\n",
    "    # Adjust splits to include all movies in the training set\n",
    "    for movie_id in missing_movie_ids:\n",
    "        found = False\n",
    "        # First try to move from the test set to the train set\n",
    "        for user_id, ratings in list(test_data.items()):\n",
    "            for rating in ratings:\n",
    "                if rating['movie_id'] == movie_id:\n",
    "                    if user_id in train_data:\n",
    "                        train_data[user_id].append(rating)\n",
    "                    else:\n",
    "                        train_data[user_id] = [rating]\n",
    "                    test_data[user_id].remove(rating)\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            # If not found in test set, try to move from the validation set\n",
    "            for user_id, ratings in list(val_data.items()):\n",
    "                for rating in ratings:\n",
    "                    if rating['movie_id'] == movie_id:\n",
    "                        if user_id in train_data:\n",
    "                            train_data[user_id].append(rating)\n",
    "                        else:\n",
    "                            train_data[user_id] = [rating]\n",
    "                        val_data[user_id].remove(rating)\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "# Function to extract prediction targets and remove them from the data split\n",
    "def extract_prediction_targets(data_split):\n",
    "    prediction_targets = []\n",
    "\n",
    "    for user_id in list(data_split.keys()):\n",
    "        ratings = data_split[user_id]\n",
    "        if ratings:\n",
    "            # Take the first rating as the target for prediction\n",
    "            rating = ratings.pop(0)\n",
    "            prediction_targets.append({\n",
    "                'id': user_id,\n",
    "                'output': rating['rating']\n",
    "            })\n",
    "            # Add the input to the first level\n",
    "            title = rating['title']\n",
    "            genres = rating['genres'].split('|')\n",
    "            prompt = f\"What rating will the user give the movie from 1 - 5 based on the movie title and categories? Title: {title} Genre: {', '.join(genres)}\"\n",
    "            data_split[user_id] = {\n",
    "                'id': user_id,\n",
    "                'input': prompt,\n",
    "                'ratings': ratings\n",
    "            }\n",
    "    \n",
    "    return prediction_targets\n",
    "\n",
    "# Extract prediction targets\n",
    "train_targets = extract_prediction_targets(train_data)\n",
    "val_targets = extract_prediction_targets(val_data)\n",
    "test_targets = extract_prediction_targets(test_data)\n",
    "\n",
    "# Define NumpyEncoder to handle numpy data types\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, np.datetime64):\n",
    "            return obj.item().isoformat()\n",
    "        else:\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# Define a function to save data to JSON\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "# Save the updated data splits and prediction targets to JSON files\n",
    "save_to_json(list(train_data.values()), 'train_data.json')\n",
    "save_to_json(list(val_data.values()), 'val_data.json')\n",
    "save_to_json(list(test_data.values()), 'test_data.json')\n",
    "\n",
    "save_to_json(train_targets, 'train_predictions.json')\n",
    "save_to_json(val_targets, 'val_predictions.json')\n",
    "save_to_json(test_targets, 'test_predictions.json')\n",
    "\n",
    "print(\"Data splits and prediction targets saved to JSON files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d9cc3-854d-441f-ad76-91c0cf9692b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to features and targets\n",
    "train_features, train_targets = convert_to_features_targets(train_data)\n",
    "val_features, val_targets = convert_to_features_targets(val_data)\n",
    "test_features, test_targets = convert_to_features_targets(test_data)\n",
    "\n",
    "# Combine titles and genres from all splits\n",
    "all_titles = [f['title'] for f in train_features + val_features + test_features]\n",
    "all_genres = [f['genres'] for f in train_features + val_features + test_features]\n",
    "\n",
    "# Fit the vectorizer on the entire corpus\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(all_titles + all_genres)\n",
    "\n",
    "# Function to prepare data for the model\n",
    "def prepare_model_data(features):\n",
    "    titles = [f['title'] for f in features]\n",
    "    genres = [f['genres'] for f in features]\n",
    "    \n",
    "    X_titles = vectorizer.transform(titles)\n",
    "    X_genres = vectorizer.transform(genres)\n",
    "    \n",
    "    # Combine these features as needed. Here we concatenate them.\n",
    "    X = hstack([X_titles, X_genres])\n",
    "    return X\n",
    "\n",
    "# Prepare the data\n",
    "X_train = prepare_model_data(train_features)\n",
    "X_val = prepare_model_data(val_features)\n",
    "X_test = prepare_model_data(test_features)\n",
    "\n",
    "# Train a simple model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, train_targets)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = model.score(X_train, train_targets)\n",
    "val_score = model.score(X_val, val_targets)\n",
    "\n",
    "print(f'Train Score: {train_score}')\n",
    "print(f'Validation Score: {val_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
