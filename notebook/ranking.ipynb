{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c3659-fbb6-42c2-88df-32d734c3440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3679f894-81bb-4e20-8239-b7b50e3fe7e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "\n",
    "# Function to load data from a JSON file\n",
    "def load_data(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def create_user_product_matrix(data):\n",
    "    user_ids = set()\n",
    "    product_ids = set()\n",
    "    matrix = {}\n",
    "    doc_id = 0  # Initialize doc_id\n",
    "\n",
    "    for user in data:\n",
    "        user_id = user['id']\n",
    "        for review in user['profile']:\n",
    "            product_id = review['productAsin']\n",
    "            user_ids.add(user_id)\n",
    "            product_ids.add(product_id)\n",
    "\n",
    "            # Set default values to \"None\" if text or title is missing or empty\n",
    "            review_title = review.get('title', 'None') or 'None'\n",
    "            review_text = review.get('text', 'None') or 'None'\n",
    "\n",
    "            matrix[(user_id, product_id)] = {\n",
    "                \"reviewTitle\": review_title,\n",
    "                \"reviewText\": review_text,\n",
    "                \"doc_id\": doc_id  # Assign doc_id\n",
    "            }\n",
    "            doc_id += 1  # Increment doc_id for the next review\n",
    "\n",
    "    user_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    product_index = {product_id: idx for idx, product_id in enumerate(product_ids)}\n",
    "\n",
    "    return matrix, user_index, product_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c1f8f5e-2e9e-4ca0-9c91-49e7066e72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_corpus_from_matrix(user_product_matrix):\n",
    "    corpus = []\n",
    "    for (user_id, product_id), review_data in user_product_matrix.items():\n",
    "        review_text = review_data['reviewText'].strip()\n",
    "        review_title = review_data['reviewTitle'].strip()\n",
    "        doc_id = review_data['doc_id']\n",
    "        if review_text:  # Ensure the review is not empty\n",
    "            corpus.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"user_id\": user_id,\n",
    "                \"product_id\": product_id,\n",
    "                \"reviewText\": review_text,\n",
    "                \"reviewTitle\": review_title,\n",
    "                \"combined\": f\"{review_title} {review_text}\"  # Combined for ranking purposes\n",
    "            })\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def compute_corpus_embeddings(corpus, contriever_model, tokenizer):\n",
    "    corpus_embeddings = {}\n",
    "    contriever_model.to(device)  # Move the model to the correct device\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_id = doc['doc_id']\n",
    "        combined_text = doc['combined']\n",
    "        \n",
    "        # Move the tokenized inputs to the correct device\n",
    "        encoded_input = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = contriever_model(**encoded_input).pooler_output  # Get the embedding vector\n",
    "        \n",
    "        corpus_embeddings[doc_id] = embedding.cpu().numpy()  # Store on CPU\n",
    "\n",
    "    return corpus_embeddings\n",
    "\n",
    "def process_embedding(emb, device=\"cpu\"):\n",
    "    if isinstance(emb, np.ndarray):\n",
    "        emb = torch.tensor(emb)\n",
    "    if emb.ndim > 1:\n",
    "        emb = emb.squeeze(0)\n",
    "    return emb.to(device)\n",
    "\n",
    "def rag_on_entire_corpus(\n",
    "    query, \n",
    "    corpus, \n",
    "    user_id, \n",
    "    product_id,  # Added product_id parameter\n",
    "    corpus_embeddings=None, \n",
    "    limit=None, \n",
    "    retrieval_method=\"contriever\", \n",
    "    filter_field=\"reviewTitle\"\n",
    "):\n",
    "    # Filter documents to exclude the user's own review(s) where both user_id and product_id match\n",
    "    filtered_corpus = [\n",
    "        doc for doc in corpus\n",
    "        if not (doc['user_id'] == user_id and doc['product_id'] == product_id)\n",
    "    ]\n",
    "    \n",
    "    if not filtered_corpus:\n",
    "        print(\"All documents are from the user for the given product; returning an empty list.\")\n",
    "        return []\n",
    "    \n",
    "    if retrieval_method == \"contriever\":\n",
    "        if corpus_embeddings is None:\n",
    "            raise ValueError(\"Corpus embeddings must be provided for Contriever retrieval.\")\n",
    "        \n",
    "        # Retrieve embeddings using doc_id\n",
    "        filtered_embeddings = []\n",
    "        for doc in filtered_corpus:\n",
    "            doc_id = doc['doc_id']\n",
    "            emb = corpus_embeddings[doc_id]\n",
    "            emb = process_embedding(emb)\n",
    "            filtered_embeddings.append(emb)\n",
    "        \n",
    "        if not filtered_embeddings:\n",
    "            print(\"No valid embeddings for filtered documents.\")\n",
    "            return []\n",
    "        \n",
    "        # Encode the query to get the query embedding\n",
    "        query_embedding = encode_for_contriever(query)\n",
    "        \n",
    "        # Perform retrieval using Contriever with precomputed embeddings\n",
    "        top_k_indices = contriever(query_embedding, filtered_embeddings, k=limit if limit else len(filtered_embeddings))\n",
    "        \n",
    "        # Retrieve the corresponding documents\n",
    "        top_k_docs = [filtered_corpus[i] for i in top_k_indices]\n",
    "        \n",
    "        # Prepare the output\n",
    "        result = [{\"reviewText\": doc[\"reviewText\"], \"reviewTitle\": doc[\"reviewTitle\"]} for doc in top_k_docs]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    else:\n",
    "        # BM25 retrieval\n",
    "        combined_documents = [doc[\"combined\"] for doc in filtered_corpus]\n",
    "        top_k_documents = bm25_retriever(query, combined_documents, k=limit if limit else len(combined_documents))\n",
    "        # Map documents back to filtered_corpus\n",
    "        top_k_docs = []\n",
    "        for doc_text in top_k_documents:\n",
    "            index = combined_documents.index(doc_text)\n",
    "            top_k_docs.append(filtered_corpus[index])\n",
    "        # Prepare the output\n",
    "        result = [{\"reviewText\": doc[\"reviewText\"], \"reviewTitle\": doc[\"reviewTitle\"]} for doc in top_k_docs]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1001809d-05f3-47a9-9cb3-4c9dd586a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgraph_rag_neighbors_ratings_only(\n",
    "    user_id, \n",
    "    product_id, \n",
    "    user_product_matrix, \n",
    "    user_index, \n",
    "    product_index, \n",
    "    query, \n",
    "    corpus_embeddings,  # Precomputed embeddings\n",
    "    corpus,             # Corpus should match embeddings by doc_id\n",
    "    limit=None, \n",
    "    retrieval_method=\"contriever\", \n",
    "    filter_field=\"reviewTitle\"\n",
    "):\n",
    "    # Retrieve the product index\n",
    "    product_idx = product_index.get(product_id)\n",
    "    if product_idx is None:\n",
    "        print(f\"Product {product_id} not found in product_index.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the user IDs who reviewed the product\n",
    "    user_ids = [uid for (uid, pid) in user_product_matrix.keys() if pid == product_id]\n",
    "    \n",
    "    # Get the corresponding user IDs and their review details for the product\n",
    "    neighbor_ratings = [\n",
    "        {\n",
    "            \"user_id\": uid,\n",
    "            \"reviewTitle\": user_product_matrix[(uid, product_id)]['reviewTitle'], \n",
    "            \"reviewText\": user_product_matrix[(uid, product_id)]['reviewText'],\n",
    "            \"doc_id\": user_product_matrix[(uid, product_id)]['doc_id']  # Include doc_id\n",
    "        }\n",
    "        for uid in user_ids\n",
    "    ]\n",
    "    \n",
    "    # Filter out the current user's own review by excluding any reviews from the user_id\n",
    "    filtered_ratings = [\n",
    "        review for review in neighbor_ratings\n",
    "        if review['user_id'] != user_id\n",
    "    ]\n",
    "    \n",
    "    if not filtered_ratings:\n",
    "        print(\"No neighbor ratings after filtering.\")\n",
    "        return []\n",
    "    \n",
    "    # If only one document is left, return it\n",
    "    if len(filtered_ratings) == 1:\n",
    "        return filtered_ratings\n",
    "    \n",
    "    if retrieval_method == \"contriever\":\n",
    "        if corpus_embeddings is None:\n",
    "            raise ValueError(\"Corpus embeddings must be provided for Contriever retrieval.\")\n",
    "        \n",
    "        # Retrieve embeddings using doc_id\n",
    "        filtered_embeddings = []\n",
    "        for review in filtered_ratings:\n",
    "            doc_id = review['doc_id']\n",
    "            emb = corpus_embeddings[doc_id]\n",
    "            emb = process_embedding(emb)\n",
    "            filtered_embeddings.append(emb)\n",
    "        \n",
    "        if not filtered_embeddings:\n",
    "            print(\"No valid embeddings for filtered documents.\")\n",
    "            return []\n",
    "        \n",
    "        # Encode the query to get the query embedding\n",
    "        query_embedding = encode_for_contriever(query)\n",
    "        \n",
    "        # Perform Contriever retrieval using the precomputed embeddings\n",
    "        top_k_indices = contriever(query_embedding, filtered_embeddings, k=limit if limit else len(filtered_embeddings))\n",
    "        \n",
    "        # Map back the indices to the filtered ratings\n",
    "        top_k_neighbors = [filtered_ratings[i] for i in top_k_indices]\n",
    "    \n",
    "    else:\n",
    "        # Combine title and text for each review for tokenization/embedding purposes\n",
    "        combined_documents = [\n",
    "            f\"{review['reviewTitle']} {review['reviewText']}\" for review in filtered_ratings\n",
    "        ]\n",
    "        # Use BM25 for tokenized retrieval\n",
    "        top_k_documents = bm25_retriever(query, combined_documents, k=limit if limit else len(combined_documents))\n",
    "        # Map documents back to filtered_ratings\n",
    "        top_k_neighbors = []\n",
    "        for doc in top_k_documents:\n",
    "            index = combined_documents.index(doc)\n",
    "            top_k_neighbors.append(filtered_ratings[index])\n",
    "    \n",
    "    return top_k_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4247bf72-4a15-4cd7-9620-9821364fa231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_all_ratings(\n",
    "    user_id, \n",
    "    product_id,  # Add product_id parameter\n",
    "    user_product_matrix, \n",
    "    query, \n",
    "    corpus_embeddings,  # Precomputed embeddings\n",
    "    corpus,             # Corpus should match embeddings by index\n",
    "    limit=None, \n",
    "    retrieval_method=\"contriever\", \n",
    "    filter_field=\"reviewTitle\"\n",
    "):\n",
    "    # Retrieve all reviews by the user\n",
    "    user_ratings = [\n",
    "        {\n",
    "            \"product_id\": pid,\n",
    "            \"reviewTitle\": user_product_matrix[(user_id, pid)].get('reviewTitle', \"None\"),\n",
    "            \"reviewText\": user_product_matrix[(user_id, pid)].get('reviewText', \"None\"),\n",
    "            \"doc_id\": user_product_matrix[(user_id, pid)]['doc_id']\n",
    "        }\n",
    "        for (uid, pid) in user_product_matrix.keys() if uid == user_id\n",
    "    ]\n",
    "\n",
    "    if not user_ratings:\n",
    "        print(f\"No reviews found for user {user_id}.\")\n",
    "        return []\n",
    "\n",
    "    # Identify the doc_id of the query item based on product_id\n",
    "    query_doc_id = None\n",
    "    for review in user_ratings:\n",
    "        if review['product_id'] == product_id:\n",
    "            query_doc_id = review['doc_id']\n",
    "            break  # Assuming the user has only one review per product\n",
    "\n",
    "    if query_doc_id is None:\n",
    "        print(f\"No matching review found for user {user_id} and product {product_id}.\")\n",
    "        # Optionally, proceed without excluding the query item\n",
    "        filtered_ratings = user_ratings\n",
    "    else:\n",
    "        # Filter out the query item using doc_id\n",
    "        filtered_ratings = [\n",
    "            review for review in user_ratings\n",
    "            if review['doc_id'] != query_doc_id\n",
    "        ]\n",
    "\n",
    "    if not filtered_ratings:\n",
    "        #print(\"No other reviews found for the user after filtering.\")\n",
    "        return []\n",
    "\n",
    "    # Combine the title and text for tokenization/embedding\n",
    "    combined_documents = [\n",
    "        f\"{review['reviewTitle']} {review['reviewText']}\" for review in filtered_ratings\n",
    "    ]\n",
    "\n",
    "    if retrieval_method == \"contriever\":\n",
    "        if corpus_embeddings is None:\n",
    "            raise ValueError(\"Corpus embeddings must be provided for Contriever retrieval.\")\n",
    "\n",
    "        # Retrieve embeddings using doc_id\n",
    "        filtered_embeddings = []\n",
    "        for review in filtered_ratings:\n",
    "            doc_id = review['doc_id']\n",
    "            emb = corpus_embeddings[doc_id]\n",
    "            emb = process_embedding(emb)\n",
    "            filtered_embeddings.append(emb)\n",
    "\n",
    "        if not filtered_embeddings:\n",
    "            print(\"No valid embeddings for filtered documents.\")\n",
    "            return []\n",
    "\n",
    "        # Encode the query to get the query embedding\n",
    "        query_embedding = encode_for_contriever(query)\n",
    "\n",
    "        # Perform Contriever-based retrieval using precomputed embeddings\n",
    "        top_k_indices = contriever(query_embedding, filtered_embeddings, k=limit if limit else len(filtered_embeddings))\n",
    "        \n",
    "        # Map back the indices to the filtered ratings\n",
    "        top_k_user_reviews = [filtered_ratings[i] for i in top_k_indices]\n",
    "    else:\n",
    "        # Use BM25 for tokenized retrieval\n",
    "        top_k_documents = bm25_retriever(query, combined_documents, k=limit if limit else len(combined_documents))\n",
    "        # Map documents back to filtered_ratings\n",
    "        top_k_user_reviews = []\n",
    "        for doc in top_k_documents:\n",
    "            index = combined_documents.index(doc)\n",
    "            top_k_user_reviews.append(filtered_ratings[index])\n",
    "\n",
    "    return top_k_user_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341b7217-59f2-4067-b682-13dfbaddf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    # Mask out padded tokens and calculate mean for non-padded tokens\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "\n",
    "def encode_for_contriever(text):\n",
    "    inputs = contriever_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        contriever_model.to(device)  # Ensure model is on the correct device\n",
    "        outputs = contriever_model(**inputs)\n",
    "        \n",
    "        # Apply mean pooling on the token embeddings with attention mask\n",
    "        embeddings = mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "    \n",
    "    return embeddings.squeeze(0).to(device)  # Ensure shape is [D]\n",
    "\n",
    "\n",
    "def contriever(query_embedding, document_embeddings, k=1):\n",
    "    # Ensure query_embedding is a tensor on the correct device\n",
    "    if not isinstance(query_embedding, torch.Tensor):\n",
    "        raise ValueError(f\"Expected query_embedding to be a tensor, but got {type(query_embedding)}\")\n",
    "    query_embedding = query_embedding.to(device)\n",
    "    \n",
    "    # Ensure document_embeddings are tensors on the correct device\n",
    "    document_embeddings = [emb.to(device) if isinstance(emb, torch.Tensor) else torch.tensor(emb).to(device) for emb in document_embeddings]\n",
    "\n",
    "    # Stack embeddings into tensor of shape [N, D]\n",
    "    document_embeddings = torch.stack(document_embeddings)\n",
    "    \n",
    "    # Calculate cosine similarities between query and document embeddings\n",
    "    similarities = torch.nn.functional.cosine_similarity(document_embeddings, query_embedding.unsqueeze(0), dim=1)\n",
    "    similarities = similarities.cpu().numpy()\n",
    "    \n",
    "    # Ensure similarities is a 1D array\n",
    "    similarities = similarities.squeeze()\n",
    "\n",
    "    # Handle potential NaN values in similarities\n",
    "    if np.isnan(similarities).any():\n",
    "        print(\"Similarities contain NaN values. Replacing NaNs with zeros.\")\n",
    "        similarities = np.nan_to_num(similarities)\n",
    "\n",
    "    # Get the indices of documents sorted by similarity\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "    top_k_indices = [int(i) for i in top_k_indices]\n",
    "    \n",
    "    return top_k_indices  # Return the indices of the top-k most similar documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc4b53c-9a04-4722-a9b7-e0a6ffb9c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using LLaMA's tokenizer and join tokens back to form a string\n",
    "def tokenize_for_bm25(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens  # Return a list of tokens for BM25\n",
    "\n",
    "def bm25_retriever(query, documents, k=1):\n",
    "    if not documents:\n",
    "        print(\"No valid documents to retrieve.\")\n",
    "        return []  # Return an empty list if all documents were filtered out\n",
    "\n",
    "    # Tokenize the filtered documents and the query using the same tokenizer\n",
    "    tokenized_documents = [tokenize_for_bm25(doc) for doc in documents]\n",
    "    \n",
    "    # Further filter out any tokenized documents that are empty\n",
    "    tokenized_documents = [tokens for tokens in tokenized_documents if tokens]\n",
    "    \n",
    "    # Check if tokenization resulted in empty documents\n",
    "    if not tokenized_documents:\n",
    "        print(\"Tokenization resulted in no valid tokens.\")\n",
    "        return []  # Return an empty list if tokenization fails\n",
    "    \n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "    tokenized_query = tokenize_for_bm25(query)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_indices = np.argsort(doc_scores)[::-1][:k]\n",
    "\n",
    "    return [documents[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd8daab-97c6-48af-834c-db24969706da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(\n",
    "    item, \n",
    "    user_product_matrix, \n",
    "    user_index, \n",
    "    product_index, \n",
    "    tokenizer, \n",
    "    corpus, \n",
    "    corpus_embeddings,  # Precomputed corpus embeddings\n",
    "    limit, \n",
    "    retrieval_method=\"contriever\", \n",
    "    filter_field=\"reviewTitle\"\n",
    "):\n",
    "    example_user_id = item['id']\n",
    "    example_product_id = item['profile'][0]['productAsin']\n",
    "    \n",
    "    query = item['profile'][0].get(filter_field, \"\") or \"None\"\n",
    "    \n",
    "    # Pass product_id to exclude the review for the current product\n",
    "    user_ratings = get_user_all_ratings(\n",
    "        user_id=example_user_id,\n",
    "        product_id=example_product_id,\n",
    "        user_product_matrix=user_product_matrix,\n",
    "        query=query,\n",
    "        corpus_embeddings=corpus_embeddings,\n",
    "        corpus=corpus,\n",
    "        limit=limit,\n",
    "        retrieval_method=retrieval_method,\n",
    "        filter_field=filter_field\n",
    "    )\n",
    "    \n",
    "    neighbor_ratings = pgraph_rag_neighbors_ratings_only(\n",
    "        user_id=example_user_id, \n",
    "        product_id=example_product_id, \n",
    "        user_product_matrix=user_product_matrix, \n",
    "        user_index=user_index, \n",
    "        product_index=product_index, \n",
    "        query=query, \n",
    "        corpus_embeddings=corpus_embeddings, \n",
    "        corpus=corpus,\n",
    "        limit=limit, \n",
    "        retrieval_method=retrieval_method, \n",
    "        filter_field=filter_field\n",
    "    )\n",
    "    \n",
    "    # Pass product_id to rag_on_entire_corpus\n",
    "    all_ratings = rag_on_entire_corpus(\n",
    "        query=query, \n",
    "        corpus=corpus, \n",
    "        user_id=example_user_id, \n",
    "        product_id=example_product_id,  # Added product_id\n",
    "        corpus_embeddings=corpus_embeddings, \n",
    "        limit=limit, \n",
    "        retrieval_method=retrieval_method, \n",
    "        filter_field=filter_field\n",
    "    )\n",
    "    \n",
    "    user_review_text = item['profile'][0].get('text', None)\n",
    "    user_review_title = item['profile'][0].get('title', None)\n",
    "\n",
    "    return {\n",
    "        \"user_id\": example_user_id,\n",
    "        \"product_id\": example_product_id,\n",
    "        \"user_review_text\": user_review_text,\n",
    "        \"user_review_title\": user_review_title,\n",
    "        \"user_ratings\": user_ratings,\n",
    "        \"neighbor_ratings\": neighbor_ratings,\n",
    "        \"all_ratings\": all_ratings\n",
    "    }\n",
    "\n",
    "def process_and_save(file_data_list, tokenizer, limit, retrieval_method=\"contriever\"):\n",
    "    for data_info in tqdm(file_data_list, desc=\"Processing datasets\", unit=\"dataset\"):\n",
    "        items = data_info['items']\n",
    "        output_file = data_info['output_file']\n",
    "        user_product_matrix = data_info['user_product_matrix']\n",
    "        user_index = data_info['user_index']\n",
    "        product_index = data_info['product_index']\n",
    "        corpus = data_info['corpus']  # Get the corpus for the current dataset\n",
    "        corpus_embeddings = data_info.get('corpus_embeddings')  # Add precomputed corpus embeddings\n",
    "\n",
    "        print(f\"Processing dataset: {output_file}\")\n",
    "        print(f\"Corpus embeddings is {'not None' if corpus_embeddings else 'None'}\")\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        # Inner loop to process each item in the current dataset\n",
    "        print(f\"\\nProcessing data for {output_file}...\")\n",
    "        for item in tqdm(items, desc=f\"Processing items in {output_file}\", unit=\"item\", leave=False):\n",
    "            result = process_item(\n",
    "                item=item, \n",
    "                user_product_matrix=user_product_matrix, \n",
    "                user_index=user_index, \n",
    "                product_index=product_index, \n",
    "                tokenizer=tokenizer, \n",
    "                corpus=corpus, \n",
    "                corpus_embeddings=corpus_embeddings,  # Pass embeddings to process_item\n",
    "                limit=limit, \n",
    "                retrieval_method=retrieval_method, \n",
    "                filter_field=\"reviewTitle\"\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        # Save the results to a JSON file\n",
    "        with open(output_file, 'w') as outfile:\n",
    "            json.dump(results, outfile, indent=4)\n",
    "        \n",
    "        print(f\"Ranked ratings saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb21fb2-3f0d-45e9-aa62-6f14dc989a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "file_path = \"../data/AmazonReview/\"\n",
    "file_name_base = \"amazon_title_generation_questions\"\n",
    "limit = 5\n",
    "ranked_suffix = f\"_ranked_k_{limit}\"\n",
    "filter_field=\"reviewTitle\"\n",
    "task_suffix = f\"{filter_field}\"\n",
    "\n",
    "# Define the suffixes for the train, test, and dev files\n",
    "file_suffixes = [\"test\", \"dev\"]\n",
    "\n",
    "# Construct the full file names with paths using the suffixes\n",
    "file_names = [os.path.join(file_path, f\"{file_name_base}_{suffix}.json\") for suffix in file_suffixes]\n",
    "\n",
    "# Load the datasets into variables using the dynamically created file names\n",
    "#train_users = load_data(file_names[0])\n",
    "test_users = load_data(file_names[0])\n",
    "dev_users = load_data(file_names[1])\n",
    "\n",
    "# Create the user-product matrices for each dataset\n",
    "#train_user_product_matrix, train_user_index, train_product_index = create_user_product_matrix(train_users)\n",
    "test_user_product_matrix, test_user_index, test_product_index = create_user_product_matrix(test_users)\n",
    "dev_user_product_matrix, dev_user_index, dev_product_index = create_user_product_matrix(dev_users)\n",
    "\n",
    "# Retrieve the entire corpus from each matrix\n",
    "#train_corpus = retrieve_corpus_from_matrix(train_user_product_matrix)\n",
    "test_corpus = retrieve_corpus_from_matrix(test_user_product_matrix)\n",
    "dev_corpus = retrieve_corpus_from_matrix(dev_user_product_matrix)\n",
    "\n",
    "# Load the Contriever model and tokenizer\n",
    "contriever_tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\")\n",
    "contriever_model = AutoModel.from_pretrained(\"facebook/contriever\")\n",
    "\n",
    "test_corpus_embeddings = compute_corpus_embeddings(test_corpus, contriever_model, contriever_tokenizer)\n",
    "dev_corpus_embeddings = compute_corpus_embeddings(dev_corpus, contriever_model, contriever_tokenizer)\n",
    "\n",
    "contriever_model.to(device)\n",
    "llama3_model = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device_map=\"auto\",)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "file_data_list = [\n",
    "    # {\n",
    "    #     'items': train_users, \n",
    "    #     'output_file': os.path.join(file_path, f\"{file_name_base}_train{ranked_suffix}.json\"), \n",
    "    #     'user_product_matrix': train_user_product_matrix, \n",
    "    #     'user_index': train_user_index, \n",
    "    #     'product_index': train_product_index,\n",
    "    #     'corpus': train_corpus\n",
    "    # },\n",
    "    \n",
    "    {\n",
    "        'items': test_users, \n",
    "        'output_file': os.path.join(file_path, f\"{file_name_base}_test{ranked_suffix}_{task_suffix}_bm25.json\"), \n",
    "        'user_product_matrix': test_user_product_matrix, \n",
    "        'user_index': test_user_index, \n",
    "        'product_index': test_product_index,\n",
    "        'corpus': test_corpus,\n",
    "        'corpus_embeddings': test_corpus_embeddings \n",
    "    },\n",
    "    {\n",
    "        'items': dev_users, \n",
    "        'output_file': os.path.join(file_path, f\"{file_name_base}_dev{ranked_suffix}_{task_suffix}_bm25.json\"), \n",
    "        'user_product_matrix': dev_user_product_matrix, \n",
    "        'user_index': dev_user_index, \n",
    "        'product_index': dev_product_index,\n",
    "        'corpus': dev_corpus,\n",
    "        'corpus_embeddings': dev_corpus_embeddings \n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21b730-b6c7-42bc-a0bc-abf6ef822fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save(file_data_list, tokenizer, limit, retrieval_method=\"bm_25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8838c59-e5d0-441c-984e-94fc901c9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Function to update user_review_text in the output file using data from user_product_matrix\n",
    "# def update_user_review_text(output_data, user_product_matrix):\n",
    "#     for entry in output_data:\n",
    "#         user_id = entry['user_id']\n",
    "#         product_id = entry['product_id']\n",
    "        \n",
    "#         # Check if the user and product exist in the matrix\n",
    "#         if (user_id, product_id) in user_product_matrix:\n",
    "#             # Retrieve and update the review text and title\n",
    "#             review_data = user_product_matrix.get((user_id, product_id), {})\n",
    "#             entry['user_review_text'] = review_data.get('reviewText', 'No review text available')\n",
    "#             entry['user_review_title'] = review_data.get('reviewTitle', 'No review title available')\n",
    "#         else:\n",
    "#             # Handle missing cases explicitly\n",
    "#             print(f\"No matching review found for user_id {user_id} and product_id {product_id}.\")\n",
    "#             entry['user_review_text'] = 'No review text available'\n",
    "#             entry['user_review_title'] = 'No review title available'\n",
    "    \n",
    "#     return output_data\n",
    "\n",
    "\n",
    "# # Load the output JSON file (amazon_title_generation.json)\n",
    "# output_file = \"../data/AmazonReview/amazon_title_generation_questions_dev_ranked_k_5_reviewText.json\" \n",
    "# with open(output_file, 'r') as file:\n",
    "#     output_data = json.load(file)\n",
    "\n",
    "# # Update the user_review_text in the output file\n",
    "# updated_data = update_user_review_text(output_data, dev_user_product_matrix)\n",
    "\n",
    "# # Save the updated data back to the output file\n",
    "# with open(output_file, 'w') as file:\n",
    "#     json.dump(updated_data, file, indent=4)\n",
    "\n",
    "# print(f\"Updated JSON saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4f6b6-2de2-4dd0-a832-64075683f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Button\n",
    "\n",
    "def shutdown_kernel():\n",
    "    from IPython.display import display\n",
    "    display(\"Shutting down kernel...\")\n",
    "    get_ipython().kernel.do_shutdown(True)\n",
    "\n",
    "shutdown_kernel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51bdec-a3b8-4972-b8cc-f5258cf90f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
