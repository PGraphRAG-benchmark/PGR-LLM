{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLunhRDoDVQY",
        "outputId": "048f95fd-08b3-4636-e3bb-20505a34945c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.51.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # FOR CJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "bQhi5971E9RA"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import time\n",
        "import torch\n",
        "import json\n",
        "from openai import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import argparse\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "pRW0KfAKdIQl"
      },
      "outputs": [],
      "source": [
        "class UserProfile:\n",
        "    def __init__(self, profile, dataset, task, ranker, split):\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.task = task\n",
        "        self.ranker = ranker\n",
        "        self.split = split\n",
        "\n",
        "        self.user_id = profile['user_id']\n",
        "        self.product_id = profile['product_id']\n",
        "        self.user_review_text = profile['user_review_text']\n",
        "        self.user_review_title = profile.get('user_review_title', None)\n",
        "\n",
        "        self.user_ratings = []\n",
        "        for review in profile['user_ratings']:\n",
        "            self.user_ratings.append({\"reviewTitle\": review.get('reviewTitle', None), \"reviewText\": review.get(\"reviewText\", None)})\n",
        "\n",
        "        self.neighbor_ratings = []\n",
        "        for review in profile['neighbor_ratings']:\n",
        "            self.neighbor_ratings.append({\"reviewTitle\": review.get('reviewTitle', None), \"reviewText\": review.get(\"reviewText\", None)})\n",
        "\n",
        "        self.all_ratings = []\n",
        "        for review in profile['all_ratings']:\n",
        "            self.all_ratings.append({\"reviewTitle\": review.get('reviewTitle', None), \"reviewText\": review.get(\"reviewText\", None)})\n",
        "\n",
        "\n",
        "    # Retrieve relevant part of main review based on task, return as formatted string\n",
        "    def get_review(self):\n",
        "\n",
        "        if self.task == \"reviewTitle\":\n",
        "            return f\"Review text: '{self.user_review_text}'\\n\"\n",
        "\n",
        "        elif self.task == \"reviewText\": # ONLY FOR AMAZON AND B2W(, and yelp?)\n",
        "            if self.dataset == \"google\":\n",
        "                raise Exception(f\"Google dataset not compatible with task: {self.task}\")\n",
        "            return f\"Review title: '{self.user_review_title}'\\n\"\n",
        "\n",
        "        elif self.task == \"reviewRating\":\n",
        "            if self.dataset == \"google\":\n",
        "                return f\"Review text: '{self.user_review_text}'\\n\"\n",
        "            return f\"Review title: '{self.user_review_title}', Review text: '{self.user_review_text}'\\n\"\n",
        "\n",
        "\n",
        "    # Retrieve related reviews from profile based on {mode} and {k}\n",
        "    def retrieve(self, mode, k):\n",
        "\n",
        "        if mode == \"user\":\n",
        "            retrieved = \"User's Own Reviews:\\n\"\n",
        "            for review in self.user_ratings[:k]:\n",
        "\n",
        "                if self.dataset == \"google\":\n",
        "                    context = f\"Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                else: # dataset == \"amazon\" or \"b2w\"\n",
        "                    context = f\"Review title: \\\"{review['reviewTitle']}\\\", Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                retrieved += context\n",
        "\n",
        "            return retrieved\n",
        "\n",
        "        elif mode == \"neighbor\":\n",
        "            retrieved = \"Other Users' Reviews:\\n\"\n",
        "            for review in self.neighbor_ratings[:k]:\n",
        "\n",
        "                if self.dataset == \"google\":\n",
        "                    context = f\"Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                else: # dataset == \"amazon\" or \"b2w\"\n",
        "                    context = f\"Review title: \\\"{review['reviewTitle']}\\\", Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                retrieved += context\n",
        "            return retrieved\n",
        "\n",
        "\n",
        "        elif mode == \"all\":\n",
        "            retrieved = \"Other Users' Reviews:\\n\"\n",
        "            for review in self.all_ratings[:k]:\n",
        "\n",
        "                if self.dataset == \"google\":\n",
        "                    context = f\"Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                else: # dataset == \"amazon\" or \"b2w\"\n",
        "                    context = f\"Review title: \\\"{review['reviewTitle']}\\\", Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                retrieved += context\n",
        "            return retrieved\n",
        "\n",
        "\n",
        "        elif mode == \"none\":\n",
        "            return \"\"\n",
        "\n",
        "    # Creates prompt for {task} on main review, with retrieval based on {mode} and {k}\n",
        "    def create_prompt(self, mode, k):\n",
        "\n",
        "        prompt = \"\"\n",
        "\n",
        "        # Initialize intro based on mode\n",
        "        if mode == \"both\":\n",
        "            intro = \"Given the following reviews from the same user and other users on the same product:\\n\"\n",
        "        elif mode == \"all\":\n",
        "            intro = \"Given the following reviews from any user on any product:\\n\"\n",
        "        elif mode == \"user\":\n",
        "            intro = \"Given the following reviews from the user on different products:\\n\"\n",
        "        elif mode == \"neighbor\":\n",
        "            intro = \"Given the following reviews from other users on the same product:\\n\"\n",
        "        elif mode == \"none\":\n",
        "            intro = \"Given only information on this review:\\n\"\n",
        "\n",
        "        prompt += intro\n",
        "\n",
        "\n",
        "        # Retrieve profiles based on mode\n",
        "        if mode == \"both\":\n",
        "            retrieved_profiles = f\"{self.retrieve('user', k)}\\n{self.retrieve('neighbor', k)}\"\n",
        "\n",
        "        else: # mode in [\"user\", \"neighbor\", \"none\", \"all\"]\n",
        "            retrieved_profiles = self.retrieve(mode, k)\n",
        "\n",
        "        prompt += retrieved_profiles\n",
        "\n",
        "\n",
        "        # Set up directions based on task\n",
        "        if self.task == \"reviewTitle\":\n",
        "            direction = \"\\nGenerate a title for the following product review from this user without any explanation: \"\n",
        "            direction += self.get_review() # append reviewText for title generation\n",
        "            direction += \"Generate the review title in 10 words or less using the format: 'Review title:'.\"\n",
        "\n",
        "        elif self.task == \"reviewText\": # ONLY FOR AMAZON AND B2W(, and yelp?)\n",
        "            direction = \"\\nGenerate a review for the following product from this user given the review title, without any explanation: \"\n",
        "            direction += self.get_review() # append reviewTitle for text generation\n",
        "            direction += \"Generate the review text using the format: 'Review text:'.\"\n",
        "\n",
        "        elif self.task == \"reviewRating\":\n",
        "            direction = \"\\nGenerate an integer rating for the following product from this user given the review title and text, without any explanation: \"\n",
        "            direction += self.get_review() # append reviewTitle and reviewText for rating generation\n",
        "            direction += \"Generate the review rating using the format: 'Rating:'.\"\n",
        "\n",
        "        prompt += direction\n",
        "\n",
        "        return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "KG3PTHoYQa18"
      },
      "outputs": [],
      "source": [
        "# Function to use GPT to generate given a {prompt}\n",
        "def gpt_call(prompt, client):\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model= \"gpt-4o-mini-20240718\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a personalized assistant, with the goal of providing users the best content using their preferences and the preferences of similar users.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.4  # temp change????????????\n",
        "            )\n",
        "\n",
        "            # Extract and print the assistant's response from the first choice\n",
        "            if response.choices:\n",
        "                generated_text = response.choices[0].message.content\n",
        "                #print(f\"Generated text: {generated_text}\") # if you want to see generations in output\n",
        "                return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in fetching the chat response: {e}\")\n",
        "            time.sleep(10)\n",
        "\n",
        "\n",
        "# CAN be used to generate a SINGLE results file, specifying mode and k\n",
        "# Function to generate {task} on {dataset}-{split} for 1 {mode} and 1 {k} with {ranker} using gpt\n",
        "def generate_gpt(data, dataset, task, ranker, split, mode, k, client=None):\n",
        "    print(f\"Processing mode: {mode} with k={k} on GPT\")\n",
        "\n",
        "    if not client:\n",
        "        client = AzureOpenAI(\n",
        "            azure_endpoint = \"https://vietgpt.openai.azure.com/\",\n",
        "            #api_key=userdata.get('AZURE_KEY'), # colab\n",
        "            api_key = os.getenv('AZURE_KEY') # not colab\n",
        "            api_version=\"2024-02-15-preview\"\n",
        "            )\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # for profile in tqdm(data, desc=f'Generating for OUTPUT-{dataset}_{split}_{task}_GPT_{ranker}-{mode}_k{k}'):\n",
        "    for profile in tqdm(data, desc=f'Generating for OUTPUT-{dataset}_{split}_{task}_LLAMA_{ranker}-{mode}_k{k}'):\n",
        "        # Store user profile in a UserProfile object\n",
        "        p = UserProfile(profile, dataset, task, ranker, split)\n",
        "\n",
        "        # Synthesize prompt from profile based on task, mode, k\n",
        "        prompt = p.create_prompt(mode, k)\n",
        "\n",
        "        # Feed prompt to GPT and store response\n",
        "        generation = gpt_call(prompt, client)\n",
        "        print(generation) # IF you want to watch as generations run\n",
        "        results.append(generation)\n",
        "\n",
        "    # save results (PROBABLY WILL CHANGE)\n",
        "    save_results(results, dataset, task, ranker, split, mode, k, \"GPT\")\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# CAN be used to generate a SINGLE results file, specifying mode and k\n",
        "# Function to generate {task} on {dataset}-{split} for 1 {mode} and 1 {k} with {ranker} using llama\n",
        "def generate_llama(data, dataset, task, ranker, split, mode, k, model=None):\n",
        "\n",
        "    # (hard coded these in for now, not sure if you want it to be adaptable)\n",
        "    max_input_length=512\n",
        "    max_output_length=256\n",
        "\n",
        "    print(f\"Processing mode: {mode} with k={k} on LLAMA\")\n",
        "\n",
        "    if not model:\n",
        "        model = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device_map=\"auto\",)\n",
        "    #if not tokenizer:\n",
        "    #    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for profile in tqdm(data, desc=f'Generating for OUTPUT-{dataset}_{split}_{task}_LLAMA_{ranker}-{mode}_k{k}'):\n",
        "        # Store user profile in a UserProfile object\n",
        "        p = UserProfile(profile, dataset, task, ranker, split)\n",
        "\n",
        "        # Synthesize prompt from profile based on task, mode, k\n",
        "        prompt = p.create_prompt(mode, k)\n",
        "\n",
        "        llama_prompt = (\n",
        "            f\"<|start_header_id|>user<|end_header_id|>\\n\"\n",
        "            f\"{prompt}\\n\"\n",
        "            f\"Do NOT generate anything else!.\\n\"\n",
        "            f\"<<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "        )\n",
        "\n",
        "        # Feed prompt to LLAMA and store response\n",
        "        generation = model(llama_prompt, max_new_tokens=max_output_length, do_sample=True, return_full_text=False)\n",
        "        print(generation) # IF you want to watch as generations run\n",
        "        results.append(generation)\n",
        "\n",
        "    # save results (PROBABLY WILL CHANGE)\n",
        "    save_results(results, dataset, task, ranker, split, mode, k, \"LLAMA\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# not necessary anymore since now partial_generate() is capable of doing full\n",
        "'''\n",
        "# Function to specify model and generate EVERYTHING for this {dataset} {task}\n",
        "def full_generate(data, dataset, task, ranker, split, model):\n",
        "    modes = [\"none\", \"all\", \"user\", \"neighbor\", \"both\"]\n",
        "    k_values = [1, 2, 4]\n",
        "\n",
        "    partial_generate(data, dataset, task, ranker, split, model, modes, k_values)\n",
        "'''\n",
        "\n",
        "# Function to generate on a subset of modes and/or a subset of k values\n",
        "# Generates everything if modes+k_values are not specified\n",
        "def partial_generate(data, dataset, task, ranker, split, model, modes=[\"none\", \"all\", \"user\", \"neighbor\", \"both\"], k_values=[1, 2, 4]):\n",
        "\n",
        "    # use gpt to generate for all mode-k combinations\n",
        "    if model == \"gpt\":\n",
        "        gpt_client = AzureOpenAI(\n",
        "            azure_endpoint = \"https://vietgpt.openai.azure.com/\",\n",
        "            #api_key=userdata.get('AZURE_KEY'), # colab\n",
        "            api_key = os.getenv('AZURE_KEY') # not colab\n",
        "            api_version=\"2024-02-15-preview\"\n",
        "            )\n",
        "\n",
        "        for k in k_values:\n",
        "            for mode in modes:\n",
        "                generate_gpt(data, dataset, task, ranker, split, mode, k, client=gpt_client)\n",
        "\n",
        "\n",
        "    # use llama to generate for all mode-k combinations\n",
        "    elif model == \"llama\":\n",
        "        llama3_model = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device_map=\"auto\",)\n",
        "        #tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "        for k in k_values:\n",
        "            for mode in modes:\n",
        "                generate_llama(data, dataset, task, ranker, split, mode, k, model=llama3_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "oqma7a-3_B4j"
      },
      "outputs": [],
      "source": [
        "# Function to load data from a (ranking) JSON file\n",
        "# example: b2w_data_dev_ranked_k_5_reviewText_bm25.json\n",
        "\n",
        "'''\n",
        "#load data using old format\n",
        "def load_data(file_path):\n",
        "    # pull filename from path\n",
        "    filename = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # parse run information from filename\n",
        "    parsed = filename.split('_') # ['b2w', 'data', 'dev', 'ranked', 'k', '5', 'reviewText', 'bm25'] # remove k5?????????? if so, use below function\n",
        "\n",
        "    dataset = parsed[0]\n",
        "    task = parsed[6]\n",
        "    ranker = parsed[7]\n",
        "    split = parsed[2]\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data, dataset, task, ranker, split\n",
        "'''\n",
        "\n",
        "#load data using new format\n",
        "def load_data(file_path):\n",
        "    # pull filename from path\n",
        "    filename = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # parse run information from filename\n",
        "    parsed = filename.split('_') # ['b2w', 'dev', 'reviewText', 'bm25']\n",
        "\n",
        "    dataset = parsed[0]\n",
        "    task = parsed[2]\n",
        "    ranker = parsed[3]\n",
        "    split = parsed[1]\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data, dataset, task, ranker, split\n",
        "\n",
        "\n",
        "def save_results(results, dataset, task, ranker, split, mode, k, model):\n",
        "    directory = './results'\n",
        "    filename = f'OUTPUT-{dataset}_{split}_{task}_{model}_{ranker}-{mode}_k{k}'\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    with open(filepath, 'w') as file:\n",
        "        json.dump(results, file, indent=4)\n",
        "\n",
        "    print(f\"{model} results for {dataset}-{split}-{task} mode='{mode}' and k={k} on ranker='{ranker}' have been saved to {filepath}\")\n",
        "\n",
        "    # below: FOR CJ\n",
        "    #!cp {filepath} /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "oZpPXEeRFCoX"
      },
      "outputs": [],
      "source": [
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Generation Pipeline\")\n",
        "    parser.add_argument('--input', type=str, required=True, help=\"Path to input data file\")\n",
        "    parser.add_argument('--model', type=str, choices=[\"gpt\", \"llama\"], required=True, help=\"Model to use ('gpt' or 'llama')\")\n",
        "    parser.add_argument('--mode', nargs='+', type=str, choices=[\"user\", \"neighbor\", \"both\"], help=\"Mode(s) to generate on. Leave empty if all modes\")\n",
        "    parser.add_argument('--k', nargs='+', type=int, help=\"K-value(s) to generate on. Leave empty if all k\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.model = args.model.lower()\n",
        "    if args.model not in ['gpt', 'llama']:\n",
        "        parser.error(\"Model must be 'gpt' or 'llama'\")\n",
        "\n",
        "    if not os.path.isfile(args.input):\n",
        "        parser.error(f\"Error: The file '{args.input}' does not exist.\")\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "raTOOqsbTPjg"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    # load args, corresponding model, data\n",
        "    args = parse_arguments()\n",
        "    data, dataset, task, ranker, split = load_data(args.input)\n",
        "\n",
        "\n",
        "    if args.mode and args.k: # specify mode and k\n",
        "        partial_generate(data, dataset, task, ranker, split, args.model, modes=args.mode, k_values=args.k)\n",
        "    elif args.mode: # specify mode\n",
        "        partial_generate(data, dataset, task, ranker, split, args.model, modes=args.mode)\n",
        "    elif args.k: # specify k\n",
        "        partial_generate(data, dataset, task, ranker, split, args.model, k_values=args.k)\n",
        "    else: # run every mode, every k\n",
        "        partial_generate(data, dataset, task, ranker, split, args.model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1CqJ0R-HcY1",
        "outputId": "574f863c-ff78-4776-fab4-5f8e959d9845"
      },
      "outputs": [],
      "source": [
        "# for testing/running notebook\n",
        "import sys\n",
        "sys.argv = ['master_generation.py', '--input', '/content/drive/Shareddrives/Intel Capstone Project/Data/Rankings/B2W/b2w_data_dev_ranked_k_5_reviewText_bm25.json', '--model', 'gpt', '--k', '4']\n",
        "\n",
        "args = parse_arguments()\n",
        "print(args)\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
