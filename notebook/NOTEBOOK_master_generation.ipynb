{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLunhRDoDVQY"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bQhi5971E9RA"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import time\n",
        "import torch\n",
        "import json\n",
        "from openai import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import argparse\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pRW0KfAKdIQl"
      },
      "outputs": [],
      "source": [
        "class UserProfile:\n",
        "    def __init__(self, profile, dataset, split, task, ranker):\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.task = task\n",
        "        self.ranker = ranker\n",
        "        self.split = split\n",
        "\n",
        "        self.user_id = profile['user_id']\n",
        "        self.product_id = profile['product_id']\n",
        "        self.user_review_text = profile['user_review_text']\n",
        "        self.user_review_title = profile.get('user_review_title', None)\n",
        "\n",
        "        self.user_ratings = [\n",
        "            {\n",
        "                \"reviewTitle\": review.get(\"reviewTitle\", None),\n",
        "                \"reviewText\": review.get(\"reviewText\", None),\n",
        "                \"reviewRating\": review.get(\"reviewRating\", None)\n",
        "            }\n",
        "            for review in profile['user_ratings']\n",
        "        ]\n",
        "\n",
        "        self.neighbor_ratings = [\n",
        "            {\n",
        "                \"reviewTitle\": review.get(\"reviewTitle\", None),\n",
        "                \"reviewText\": review.get(\"reviewText\", None),\n",
        "                \"reviewRating\": review.get(\"reviewRating\", None)\n",
        "            }\n",
        "            for review in profile['neighbor_ratings']\n",
        "        ]\n",
        "\n",
        "        self.random_review = profile['random_review']\n",
        "\n",
        "\n",
        "        '''\n",
        "        # deprecated\n",
        "        self.all_ratings = []\n",
        "        for review in profile['all_ratings']:\n",
        "            self.all_ratings.append({\"reviewTitle\": review.get('reviewTitle', None), \"reviewText\": review.get(\"reviewText\", None)})\n",
        "        '''\n",
        "\n",
        "\n",
        "\n",
        "    # Retrieve relevant part of main review based on task, return as formatted string\n",
        "    def get_review(self):\n",
        "\n",
        "        if self.task == \"reviewTitle\":\n",
        "            return f\"Review text: '{self.user_review_text}'\\n\"\n",
        "\n",
        "        elif self.task == \"reviewText\":\n",
        "            return f\"Review title: '{self.user_review_title}'\\n\"\n",
        "\n",
        "        elif self.task == \"reviewRating\":\n",
        "            return f\"Review title: '{self.user_review_title}', Review text: '{self.user_review_text}'\\n\"\n",
        "\n",
        "\n",
        "    # Retrieve related reviews from profile based on {mode} and {k}\n",
        "    def retrieve(self, mode, k):\n",
        "\n",
        "        if mode == \"user\":\n",
        "            retrieved = \"User's Own Reviews:\\n\"\n",
        "            for review in self.user_ratings[:k]:\n",
        "                rating = \"\"\n",
        "                if review and self.task == \"reviewRating\":\n",
        "                    rating += f\", Review rating: {review['reviewRating']}\"\n",
        "\n",
        "                context = f\"Review title: \\\"{review['reviewTitle']}\\\", Review text: \\\"{review['reviewText']}\\\"{rating}\\n\"\n",
        "                retrieved += context\n",
        "\n",
        "            return retrieved\n",
        "\n",
        "        elif mode == \"neighbor\":\n",
        "            retrieved = \"Other Users' Reviews:\\n\"\n",
        "            for review in self.neighbor_ratings[:k]:\n",
        "                rating = \"\"\n",
        "                if review and self.task == \"reviewRating\":\n",
        "                    rating += f\", Review rating: {review['reviewRating']}\"\n",
        "\n",
        "                context = f\"Review title: \\\"{review['reviewTitle']}\\\", Review text: \\\"{review['reviewText']}\\\"{rating}\\n\"\n",
        "                retrieved += context\n",
        "\n",
        "            return retrieved\n",
        "\n",
        "        elif mode == \"random\":\n",
        "            retrieved = \"Random Review:\\n\"\n",
        "            review = self.random_review\n",
        "\n",
        "            rating = \"\"\n",
        "            if review and self.task == \"reviewRating\":\n",
        "                rating += f\", Review rating: {review['reviewRating']}\"\n",
        "\n",
        "            context = f\"Review title: \\\"{review['reviewTitle']}\\\", Review text: \\\"{review['reviewText']}\\\"{rating}\\n\"\n",
        "            retrieved += context\n",
        "\n",
        "            return retrieved\n",
        "\n",
        "        elif mode == \"none\":\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "\n",
        "        '''\n",
        "        # deprecated\n",
        "        elif mode == \"all\":\n",
        "            retrieved = \"Other Users' Reviews:\\n\"\n",
        "            for review in self.all_ratings[:k]:\n",
        "\n",
        "                if self.dataset == \"google\":\n",
        "                    context = f\"Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                else: # dataset == \"amazon\" or \"b2w\"\n",
        "                    context = f\"Review title: \\\"{review['reviewTitle']}\\\", Review text: \\\"{review['reviewText']}\\\"\\n\"\n",
        "                retrieved += context\n",
        "            return retrieved\n",
        "        '''\n",
        "\n",
        "    # Creates prompt for {task} on main review, with retrieval based on {mode} and {k}\n",
        "    def create_prompt(self, mode, k):\n",
        "\n",
        "        prompt = \"\"\n",
        "\n",
        "        # Initialize intro based on mode\n",
        "        if mode == \"both\":\n",
        "            intro = \"Given the following reviews from the same user and other users on the same product:\\n\"\n",
        "        # elif mode == \"all\":\n",
        "        #     intro = \"Given the following reviews from any user on any product:\\n\"\n",
        "        elif mode == \"random\":\n",
        "            intro = \"Given a random review from any user on any product:\\n\"\n",
        "        elif mode == \"user\":\n",
        "            intro = \"Given the following reviews from the user on different products:\\n\"\n",
        "        elif mode == \"neighbor\":\n",
        "            intro = \"Given the following reviews from other users on the same product:\\n\"\n",
        "        elif mode == \"none\":\n",
        "            intro = \"Given only information on this review:\\n\"\n",
        "\n",
        "        prompt += intro\n",
        "\n",
        "\n",
        "        # Retrieve profiles based on mode\n",
        "        if mode == \"both\":\n",
        "            retrieved_profiles = f\"{self.retrieve('user', k)}\\n{self.retrieve('neighbor', k)}\"\n",
        "\n",
        "        else: # mode in [\"user\", \"neighbor\", \"none\", \"all\"]\n",
        "            retrieved_profiles = self.retrieve(mode, k)\n",
        "\n",
        "        prompt += retrieved_profiles\n",
        "\n",
        "\n",
        "        # Set up directions based on task\n",
        "        if self.task == \"reviewTitle\":\n",
        "            direction = \"\\nGenerate a title for the following product review from this user without any explanation: \"\n",
        "            direction += self.get_review() # append reviewText for title generation\n",
        "            direction += \"Generate the review title in 10 words or less using the format: 'Review title:'.\"\n",
        "\n",
        "        elif self.task == \"reviewText\": # ONLY FOR AMAZON AND B2W(, and yelp?)\n",
        "            direction = \"\\nGenerate a review for the following product from this user given the review title, without any explanation: \"\n",
        "            direction += self.get_review() # append reviewTitle for text generation\n",
        "            direction += \"Generate the review text using the format: 'Review text:'.\"\n",
        "\n",
        "        elif self.task == \"reviewRating\":\n",
        "            direction = \"\\nGenerate an integer rating from 1-5 for the following product from this user given the review title and text, without any explanation: \"\n",
        "            direction += self.get_review() # append reviewTitle and reviewText for rating generation\n",
        "            direction += \"Generate the review rating using the format: 'Rating:'.\"\n",
        "\n",
        "        prompt += direction\n",
        "\n",
        "        return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KG3PTHoYQa18"
      },
      "outputs": [],
      "source": [
        "# Function to use GPT to generate given a {prompt}\n",
        "def gpt_call(prompt, client):\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model= \"gpt-4o-mini-20240718\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a personalized assistant, with the goal of providing users the best content using their preferences and the preferences of similar users.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.4  # temp change????????????\n",
        "            )\n",
        "\n",
        "            # Extract and print the assistant's response from the first choice\n",
        "            if response.choices:\n",
        "                generated_text = response.choices[0].message.content\n",
        "                #print(f\"Generated text: {generated_text}\") # if you want to see generations in output\n",
        "                return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in fetching the chat response: {e}\")\n",
        "            time.sleep(10)\n",
        "\n",
        "\n",
        "# CAN be used to generate a SINGLE results file, specifying mode and k\n",
        "# Function to generate {task} on {dataset}-{split} for 1 {mode} and 1 {k} with {ranker} using gpt\n",
        "def generate_gpt(data, dataset, split, task, ranker, mode, k, client=None):\n",
        "    print(f\"Processing mode: {mode} with k={k} on GPT\")\n",
        "\n",
        "    if not client:\n",
        "        client = AzureOpenAI(\n",
        "            azure_endpoint = \"https://vietgpt.openai.azure.com/\",\n",
        "            api_key=userdata.get('AZURE_KEY'),\n",
        "            api_version=\"2024-02-15-preview\"\n",
        "            )\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for profile in tqdm(data, desc=f'Generating for OUTPUT-{dataset}_{split}_{task}_GPT_{ranker}-{mode}_k{k}'):\n",
        "        # Store user profile in a UserProfile object\n",
        "        p = UserProfile(profile, dataset, split, task, ranker)\n",
        "\n",
        "        # Synthesize prompt from profile based on task, mode, k\n",
        "        prompt = p.create_prompt(mode, k)\n",
        "\n",
        "        # Feed prompt to GPT and store response\n",
        "        generation = gpt_call(prompt, client)\n",
        "        print(generation) # IF you want to watch as generations run\n",
        "        results.append(generation)\n",
        "\n",
        "    # save results (PROBABLY WILL CHANGE)\n",
        "    save_results(results, dataset, split, task, ranker, mode, k, \"GPT\")\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# CAN be used to generate a SINGLE results file, specifying mode and k\n",
        "# Function to generate {task} on {dataset}-{split} for 1 {mode} and 1 {k} with {ranker} using llama\n",
        "def generate_llama(data, dataset, split, task, ranker, mode, k, model=None):\n",
        "\n",
        "    # (hard coded these in for now, not sure if you want it to be adaptable)\n",
        "    max_input_length=512\n",
        "    max_output_length=256\n",
        "\n",
        "    print(f\"Processing mode: {mode} with k={k} on LLAMA\")\n",
        "\n",
        "    if not model:\n",
        "        model = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device_map=\"auto\",)\n",
        "    #if not tokenizer:\n",
        "    #    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for profile in tqdm(data, desc=f'Generating for OUTPUT-{dataset}_{split}_{task}_LLAMA_{ranker}-{mode}_k{k}'):\n",
        "        # Store user profile in a UserProfile object\n",
        "        p = UserProfile(profile, dataset, split, task, ranker)\n",
        "\n",
        "        # Synthesize prompt from profile based on task, mode, k\n",
        "        prompt = p.create_prompt(mode, k)\n",
        "\n",
        "        llama_prompt = (\n",
        "            f\"<|start_header_id|>user<|end_header_id|>\\n\"\n",
        "            f\"{prompt}\\n\"\n",
        "            f\"Do NOT generate anything else!.\\n\"\n",
        "            f\"<<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "        )\n",
        "\n",
        "        # Feed prompt to LLAMA and store response\n",
        "        generation = model(llama_prompt, max_new_tokens=max_output_length, do_sample=True, return_full_text=False)\n",
        "        print(generation) # IF you want to watch as generations run\n",
        "        results.append(generation)\n",
        "\n",
        "    # save results (PROBABLY WILL CHANGE)\n",
        "    save_results(results, dataset, split, task, ranker, mode, k, \"LLAMA\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# not necessary anymore since now partial_generate() is capable of doing full\n",
        "'''\n",
        "# Function to specify model and generate EVERYTHING for this {dataset} {task}\n",
        "def full_generate(data, dataset, split, task, ranker, model):\n",
        "    modes = [\"none\", \"all\", \"user\", \"neighbor\", \"both\"]\n",
        "    k_values = [1, 2, 4]\n",
        "\n",
        "    partial_generate(data, dataset, split, task, ranker, model, modes, k_values)\n",
        "'''\n",
        "\n",
        "# Function to generate on a subset of modes and/or a subset of k values\n",
        "# Generates everything if modes+k_values are not specified\n",
        "# def partial_generate(data, dataset, split, task, ranker, model, modes=[\"none\", \"all\", \"user\", \"neighbor\", \"both\"], k_values=[1, 2, 4]):\n",
        "def partial_generate(data, dataset, split, task, ranker, model, modes=[\"none\", \"user\", \"neighbor\", \"both\", \"random\"], k_values=[1, 2, 4]):\n",
        "\n",
        "    # use gpt to generate for all mode-k combinations\n",
        "    if model == \"gpt\":\n",
        "        gpt_client = AzureOpenAI(\n",
        "            azure_endpoint = \"https://vietgpt.openai.azure.com/\",\n",
        "            api_key=userdata.get('AZURE_KEY'),\n",
        "            api_version=\"2024-02-15-preview\"\n",
        "            )\n",
        "\n",
        "        for k in k_values:\n",
        "            for mode in modes:\n",
        "                if mode in ['none', 'random'] and k in [2, 4]:\n",
        "                    continue\n",
        "                generate_gpt(data, dataset, split, task, ranker, mode, k, client=gpt_client)\n",
        "\n",
        "\n",
        "    # use llama to generate for all mode-k combinations\n",
        "    elif model == \"llama\":\n",
        "        llama3_model = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device_map=\"auto\",)\n",
        "        #tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "        for k in k_values:\n",
        "            for mode in modes:\n",
        "                if mode in ['none', 'random'] and k in [2, 4]:\n",
        "                    continue\n",
        "                generate_llama(data, dataset, split, task, ranker, mode, k, model=llama3_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oqma7a-3_B4j"
      },
      "outputs": [],
      "source": [
        "# Function to load data from a (ranking) JSON file\n",
        "# example: RANKING-b2w_dev_reviewText_bm25.json\n",
        "\n",
        "#load data using new format\n",
        "def load_data(file_path):\n",
        "    # pull filename from path\n",
        "    filename = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Remove the 'RANKING-' prefix\n",
        "    if filename.startswith(\"RANKING-\"):\n",
        "        filename = filename[len(\"RANKING-\"):]\n",
        "\n",
        "    # parse run information from filename\n",
        "    parsed = filename.split('_') # ['b2w', 'dev', 'reviewText', 'bm25']\n",
        "\n",
        "    dataset = parsed[0]\n",
        "    split = parsed[1]\n",
        "    task = parsed[2]\n",
        "    ranker = parsed[3]\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data, dataset, split, task, ranker\n",
        "\n",
        "\n",
        "def save_results(results, dataset, split, task, ranker, mode, k, model):\n",
        "    directory = './results'\n",
        "    filename = f'OUTPUT-{dataset}_{split}_{task}_{model}_{ranker}-{mode}_k{k}'\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    with open(filepath, 'w') as file:\n",
        "        json.dump(results, file, indent=4)\n",
        "\n",
        "    print(f\"{model} results for {dataset}-{split}-{task} mode='{mode}' and k={k} on ranker='{ranker}' have been saved to {filepath}\")\n",
        "\n",
        "    # below: FOR CJ\n",
        "    #!cp {filepath} /content/drive/MyDrive/intel/results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oZpPXEeRFCoX"
      },
      "outputs": [],
      "source": [
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Generation Pipeline\")\n",
        "    parser.add_argument('--input', type=str, required=True, help=\"Path to input data file\")\n",
        "    parser.add_argument('--model', type=str, choices=[\"gpt\", \"llama\"], required=True, help=\"Model to use ('gpt' or 'llama')\")\n",
        "    parser.add_argument('--mode', nargs='+', type=str, choices=[\"none\", \"random\", \"user\", \"neighbor\", \"both\"], help=\"Mode(s) to generate on. Leave empty if all modes\")\n",
        "    parser.add_argument('--k', nargs='+', type=int, help=\"K-value(s) to generate on. Leave empty if all k\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.model = args.model.lower()\n",
        "    if args.model not in ['gpt', 'llama']:\n",
        "        parser.error(\"Model must be 'gpt' or 'llama'\")\n",
        "\n",
        "    if not os.path.isfile(args.input):\n",
        "        parser.error(f\"Error: The file '{args.input}' does not exist.\")\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "raTOOqsbTPjg"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    # load args, corresponding model, data\n",
        "    args = parse_arguments()\n",
        "    data, dataset, split, task, ranker = load_data(args.input)\n",
        "\n",
        "\n",
        "    if args.mode and args.k: # specify mode and k\n",
        "        partial_generate(data, dataset, split, task, ranker, args.model, modes=args.mode, k_values=args.k)\n",
        "    elif args.mode: # specify mode\n",
        "        partial_generate(data, dataset, split, task, ranker, args.model, modes=args.mode)\n",
        "    elif args.k: # specify k\n",
        "        partial_generate(data, dataset, split, task, ranker, args.model, k_values=args.k)\n",
        "    else: # run every mode, every k\n",
        "        partial_generate(data, dataset, split, task, ranker, args.model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1CqJ0R-HcY1",
        "outputId": "7c17b85a-f564-4811-d1a3-dca205690a8d"
      },
      "outputs": [],
      "source": [
        "# for testing/running notebook\n",
        "import sys\n",
        "sys.argv = ['master_generation.py', '--input', '/content/drive/Shareddrives/Intel Capstone Project/Data/Rankings/AmazonReview/RANKING-amazon_dev_reviewRating_bm25.json', '--model', 'gpt', '--mode', 'user', 'neighbor', 'both', 'random', '--k', '1']\n",
        "\n",
        "args = parse_arguments()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha4ZCmnSmgM9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
